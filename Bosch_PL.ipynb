{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Pickle files/Train_ValCnts.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2696808d87d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;31m#ReadTrainData_newFeatures()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0mtrainData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Pickle files/Train_ValCnts.pk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;31m#    xgb = trainModel(trainData)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2696808d87d7>\u001b[0m in \u001b[0;36mOpenFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Pickle files/Train_ValCnts.pk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "\n",
    "def OpenFile(file):\n",
    "    with open(file, 'rb') as fi:\n",
    "        data = pickle.load(fi)\n",
    "    return data\n",
    "    \n",
    "@jit\n",
    "def mcc(tp, tn, fp, fn):\n",
    "    sup = tp * tn - fp * fn\n",
    "    inf = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    if inf==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sup / np.sqrt(inf)\n",
    "\n",
    "@jit\n",
    "def eval_mcc(y_true, y_prob):\n",
    "    idx = np.argsort(y_prob)\n",
    "    y_true_sort = y_true[idx]\n",
    "    n = y_true.shape[0]\n",
    "    nump = 1.0 * np.sum(y_true) # number of positive\n",
    "    numn = n - nump # number of negative\n",
    "    tp = nump\n",
    "    tn = 0.0\n",
    "    fp = numn\n",
    "    fn = 0.0\n",
    "    best_mcc = 0.0\n",
    "    prev_proba = -1\n",
    "    mccs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        # all items with idx < i are predicted negative while others are predicted positive\n",
    "        # only evaluate mcc when probability changes\n",
    "        proba = y_prob[idx[i]]\n",
    "        if proba != prev_proba:\n",
    "            prev_proba = proba\n",
    "            new_mcc = mcc(tp, tn, fp, fn)\n",
    "            if new_mcc >= best_mcc:\n",
    "                best_mcc = new_mcc\n",
    "        mccs[i] = new_mcc\n",
    "        if y_true_sort[i] == 1:\n",
    "            tp -= 1.0\n",
    "            fn += 1.0\n",
    "        else:\n",
    "            fp -= 1.0\n",
    "            tn += 1.0\n",
    "    return best_mcc\n",
    "        \n",
    "def mcc_eval(y_prob, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    best_mcc = eval_mcc(y_true, y_prob)\n",
    "    return 'MCC', best_mcc\n",
    "    \n",
    "def ReadTestData(files, cols):\n",
    "    directory = 'test/'\n",
    "    testData = None\n",
    "    for j,f in enumerate(files):\n",
    "        print(\"file: \" + f)\n",
    "        subset = None\n",
    "        for i, chunk in enumerate(pd.read_csv(directory + f, \\\n",
    "                                                chunksize = 50000, \\\n",
    "                                                usecols=cols, \\\n",
    "                                                low_memory = False)):\n",
    "            print(\"chunk \" + str(i))\n",
    "            if subset is None:\n",
    "                subset = chunk.copy()\n",
    "            else:\n",
    "                subset = pd.concat([subset, chunk])\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        if testData is None:\n",
    "            testData = subset.copy()\n",
    "        else:\n",
    "            testData = pd.merge(testData, subset.copy(), on=\"Id\")\n",
    "        del subset\n",
    "        gc.collect()\n",
    "    with open('testData_datesOnly.pk', 'wb') as fi:\n",
    "        pickle.dump(testData, fi)    \n",
    "        \n",
    "def ReadTrainData_Resp1():\n",
    "    Directory = 'train/'\n",
    "    Files = ['train_numeric.csv', \\\n",
    "                  'train_categorical.csv', \\\n",
    "                  'train_date.csv']\n",
    "\n",
    "    trainData = None\n",
    "    maskArr = None\n",
    "    for j,f in enumerate(Files):\n",
    "        print(\"file: \" + f)\n",
    "        subset = None\n",
    "        for i, chunk in enumerate(pd.read_csv(Directory + f, \\\n",
    "                                                chunksize = 50000, \\\n",
    "                                                low_memory = False)):\n",
    "            print(\"chunk \" + str(i))\n",
    "            if j == 0:\n",
    "                chunk = chunk[chunk['Response'] == 1]\n",
    "            if subset is None:\n",
    "                subset = chunk.copy()\n",
    "            else:\n",
    "                subset = pd.concat([subset, chunk])\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "            if j == 0:\n",
    "                if subset.size > 0:\n",
    "                    if maskArr is None:\n",
    "                        maskArr = np.array(subset['Id'])\n",
    "                    else:\n",
    "                        maskArr = np.append(maskArr, subset['Id'])\n",
    "            else:\n",
    "                subset = subset[subset['Id'].isin(maskArr)]\n",
    "        if trainData is None:\n",
    "            trainData = subset.copy()\n",
    "        else:\n",
    "            trainData = pd.merge(trainData, subset.copy(), on=\"Id\")\n",
    "        del subset\n",
    "        gc.collect()\n",
    "    with open('Response_1s.pk', 'wb') as fi:\n",
    "        pickle.dump(trainData, fi)    \n",
    "        \n",
    "def ReadTrainData_newFeatures():\n",
    "    Directory = 'train/'\n",
    "    Files = ['train_numeric.csv', \\\n",
    "                  'train_categorical.csv', \\\n",
    "                  'train_date.csv']\n",
    "\n",
    "    trainData = None\n",
    "    for j,f in enumerate(Files):\n",
    "        print(\"file: \" + f)\n",
    "        subset = None\n",
    "        for i, chunk in enumerate(pd.read_csv(Directory + f, \\\n",
    "                                                chunksize = 50000, \\\n",
    "                                                low_memory = False)):\n",
    "            print(\"chunk \" + str(i))\n",
    "            sumCnt = chunk.notnull().sum(axis=1)\n",
    "            sumCnt[:] = [x - 1 for x in sumCnt]\n",
    "            if j == 0:\n",
    "                sumCnt[:] = [x - 1 for x in sumCnt]\n",
    "                chunkMod = pd.DataFrame({\"Id\": chunk.Id.values, \\\n",
    "                               \"NumericCnt\": sumCnt, \\\n",
    "                               \"Response\": chunk.Response.values}) \n",
    "            elif j == 1:\n",
    "                chunkMod = pd.DataFrame({\"Id\": chunk.Id.values, \\\n",
    "                               \"CategoricalCnt\": sumCnt})\n",
    "            elif j == 2:\n",
    "                chunkMod = pd.DataFrame({\"Id\": chunk.Id.values, \\\n",
    "                               \"DateCnt\": sumCnt})\n",
    "            if subset is None:\n",
    "                subset = chunkMod.copy()\n",
    "            else:\n",
    "                subset = pd.concat([subset, chunkMod])\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "            \n",
    "        if trainData is None:\n",
    "            trainData = subset.copy()\n",
    "        else:\n",
    "            trainData = pd.merge(trainData, subset.copy(), on=\"Id\")\n",
    "        del subset\n",
    "        gc.collect()\n",
    "    with open('Train_ValCnts.pk', 'wb') as fi:\n",
    "        pickle.dump(trainData, fi) \n",
    "\n",
    "def ReadTrainData_byDate(cols):\n",
    "    trainData = None\n",
    "    subset = None\n",
    "    for i, chunk in enumerate(pd.read_csv('train/train_date.csv', \\\n",
    "                                                chunksize = 50000, \\\n",
    "                                                usecols=cols, \\\n",
    "                                                low_memory = False)):\n",
    "        print(\"chunk \" + str(i))\n",
    "        dateCols = np.setdiff1d(chunk.columns, ['Id'])\n",
    "        chunk['Start_Time'] = chunk[dateCols].min(axis=1).values\n",
    "        if subset is None:\n",
    "            subset = chunk.copy()\n",
    "        else:\n",
    "            subset = pd.concat([subset, chunk])\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    if trainData is None:\n",
    "        trainData = subset.copy()\n",
    "    else:\n",
    "        trainData = pd.merge(trainData, subset.copy(), on=\"Id\")\n",
    "    del subset\n",
    "    gc.collect()\n",
    "   \n",
    "                                                    \n",
    "    subset = None                                                \n",
    "    for i, chunk in enumerate(pd.read_csv('train/train_numeric.csv', \\\n",
    "                                                chunksize = 50000, \\\n",
    "                                                usecols=['Response', 'Id'], \\\n",
    "                                                low_memory = False)):\n",
    "        print(\"chunk \" + str(i))\n",
    "        chunk = chunk[['Response', 'Id']]\n",
    "        if subset is None:\n",
    "            subset = chunk.copy()\n",
    "        else:\n",
    "            subset = pd.concat([subset, chunk])\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    if trainData is None:\n",
    "        trainData = subset.copy()\n",
    "    else:\n",
    "        trainData = pd.merge(trainData, subset.copy(), on=\"Id\")\n",
    "    del subset\n",
    "    gc.collect()\n",
    "    with open('Full_byDate_trainData.pk', 'wb') as fi:\n",
    "        pickle.dump(trainData, fi)\n",
    "    return trainData\n",
    "\n",
    "def ReadDateTrainData(cols):\n",
    "    Directory = 'train/'\n",
    "    Files = ['train_date.csv']\n",
    "    trainData = None\n",
    "    for j,f in enumerate(Files):\n",
    "        print(\"file: \" + f)\n",
    "        subset = None\n",
    "        for i, chunk in enumerate(pd.read_csv(Directory + f, \\\n",
    "                                                chunksize = 50000, \\\n",
    "                                                usecols=cols, \\\n",
    "                                                low_memory = False)):\n",
    "            print(\"chunk \" + str(i))\n",
    "            if subset is None:\n",
    "                subset = chunk.copy()\n",
    "            else:\n",
    "                subset = pd.concat([subset, chunk])\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        if trainData is None:\n",
    "            trainData = subset.copy()\n",
    "        else:\n",
    "            trainData = pd.merge(trainData, subset.copy(), on=\"Id\")\n",
    "        del subset\n",
    "        gc.collect()\n",
    "    with open('Date_Data.pk', 'wb') as fi:\n",
    "        pickle.dump(trainData, fi)\n",
    "    return trainData\n",
    "\n",
    "def ReadCatTrainData():\n",
    "    Directory = 'train/'\n",
    "    Files = ['train_categorical.csv']\n",
    "    trainData = None\n",
    "    for j,f in enumerate(Files):\n",
    "        print(\"file: \" + f)\n",
    "        subset = None\n",
    "        for i, chunk in enumerate(pd.read_csv(Directory + f, \\\n",
    "                                                chunksize = 50000, \\\n",
    "                                                low_memory = False)):\n",
    "            print(\"chunk \" + str(i))\n",
    "            if subset is None:\n",
    "                subset = chunk.copy()\n",
    "            else:\n",
    "                subset = pd.concat([subset, chunk])\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        if trainData is None:\n",
    "            trainData = subset.copy()\n",
    "        else:\n",
    "            trainData = pd.merge(trainData, subset.copy(), on=\"Id\")\n",
    "        del subset\n",
    "        gc.collect()\n",
    "    with open('Numeric_Data.pk', 'wb') as fi:\n",
    "        pickle.dump(trainData, fi)\n",
    "    return trainData\n",
    "    \n",
    "def trainModel(trainData):\n",
    "    trainData = trainData.sort_values(by=['Start_Time', 'Id'], ascending=True)\n",
    "    ytrain = trainData.pop('Response')\n",
    "    #trainData.pop('Id')\n",
    "    prior = np.sum(ytrain) / (1.*len(ytrain))\n",
    "    xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 4,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 2,\n",
    "    'eval_metric': 'auc',\n",
    "    'base_score': prior }   \n",
    "    \n",
    "    xgdmat = xgb.DMatrix(trainData, label=ytrain)\n",
    "    cv_xgb = xgb.cv(params = xgb_params, dtrain = xgdmat, \\\n",
    "                    num_boost_round=10, \\\n",
    "                    nfold = 4, \\\n",
    "                    seed = 0, \\\n",
    "                    stratified = True, \\\n",
    "                    feval=mcc_eval, \\\n",
    "                    maximize=True,  \\\n",
    "                early_stopping_rounds = 1, \\\n",
    "                verbose_eval=1, show_stdv=True )\n",
    "    return cv_xgb\n",
    "#    \n",
    "#    train_xgb = xgb.train(params = our_params, dtrain = xgdmat, \\\n",
    "#                    feval=mcc_eval, \\\n",
    "#                    maximize=True)\n",
    "                \n",
    "#    testFiles = ['test_date.csv']\n",
    "#    testCols = dateCols\n",
    "#    ReadTestData(testFiles, testCols)\n",
    "                \n",
    "#    testData = OpenFile('testData_datesOnly.pk')\n",
    "#\n",
    "#    testdmat = xgb.DMatrix(testData)\n",
    "#    y_pred = train_xgb.predict(testdmat)\n",
    "#    thresholdVal = .3\n",
    "#    low_val = y_pred < thresholdVal\n",
    "#    high_val = ~low_val\n",
    "#    y_pred[low_val] = 0\n",
    "#    y_pred[high_val] = 1\n",
    "#    y_pred = y_pred.astype(int)\n",
    "#    submission = pd.DataFrame({\"Id\": testData.Id.values, \\\n",
    "#                               \"Response\": y_pred})\n",
    "#    submission[['Id', 'Response']].to_csv('xgbsubmission.csv', \\\n",
    "#                                            index=False) \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print('Started')\n",
    "\n",
    "    #ReadTrainData_newFeatures()\n",
    "    trainData = OpenFile('/Pickle files/Train_ValCnts.pk')\n",
    "#    xgb = trainModel(trainData)\n",
    "\n",
    "#    CatCnt = np.array(trainData.CategoricalCnt)\n",
    "#    NumericCnt = np.array(trainData.NumericCnt)\n",
    "#    DateCnt = np.array(trainData.DateCnt)\n",
    "#    Resp1 = np.array(trainData.Response == 1)\n",
    "#    CatCnt2 = CatCnt[Resp1]\n",
    "#    NumericCnt2 = NumericCnt[Resp1]\n",
    "#    DateCnt2 = DateCnt[Resp1]\n",
    "    \n",
    "#    trainData = OpenFile('Response_1s.pk')    \n",
    "#    dateData = trainData[trainData.columns[-1156:]].copy()\n",
    "#    dateData = dateData.dropna(axis=1, how='all')\n",
    "#    dateData = dateData.T.drop_duplicates().T \n",
    "#    dateCols = np.array(dateData.columns)\n",
    "#    dateCols = np.append(['Id'], dateCols)\n",
    "    #ReadTrainData_byDate(dateCols)\n",
    "    #trainData = OpenFile('Full_byDate_trainData.pk') \n",
    "    #ReadNumericTrainData()\n",
    "    #trainData = OpenFile('Numeric_Data.pk')\n",
    "    #xgb = trainModel(trainData)\n",
    "    #ReadDateTrainData(dateCols)\n",
    "#    with open('Date_Data.pk', 'rb') as fi:\n",
    "#        trainDateData = pickle.load(fi)\n",
    "#    q = trainDateData.iloc[:,:].values\n",
    "#    q=q[~np.isnan(q)]\n",
    "#    z = plt.hist(q, bins=20)\n",
    "#    p = dateData.iloc[:,:].values\n",
    "#    p=p[~np.isnan(p)]\n",
    "#    y = plt.hist(p, bins=20)\n",
    "#    z=z[0]\n",
    "#    y=y[0]\n",
    "#    zz = [a_i - b_i for a_i, b_i in zip(z, y)]\n",
    "#    xx = [a_i / b_i for a_i, b_i in zip(y, zz)]\n",
    "#    plt.plot(xx)\n",
    "\n",
    "    \n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HI #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dir>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\farha\\\\Documents\\\\Python\\\\Kaggle\\\\Bosch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\Pickle_files\\\\Train_ValCnts.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-47ece60ad617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\Pickle_files\\\\Train_ValCnts.pk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-2696808d87d7>\u001b[0m in \u001b[0;36mOpenFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\Pickle_files\\\\Train_ValCnts.pk'"
     ]
    }
   ],
   "source": [
    "trainData = OpenFile('\\\\Pickle_files\\\\Train_ValCnts.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\Train_ValCnts.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b5ac94d5c27b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\Train_ValCnts.pk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-2696808d87d7>\u001b[0m in \u001b[0;36mOpenFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\Train_ValCnts.pk'"
     ]
    }
   ],
   "source": [
    "trainData = OpenFile('\\\\Train_ValCnts.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\farha\\\\Documents\\\\Python\\\\Kaggle\\\\Bosch'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.abspath('Train_ValCnts.pk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\farha\\\\Documents\\\\Python\\\\Kaggle\\\\Bosch\\\\Train_ValCnts.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8487925e2a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\farha\\\\Documents\\\\Python\\\\Kaggle\\\\Bosch\\\\Train_ValCnts.pk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-2696808d87d7>\u001b[0m in \u001b[0;36mOpenFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\farha\\\\Documents\\\\Python\\\\Kaggle\\\\Bosch\\\\Train_ValCnts.pk'"
     ]
    }
   ],
   "source": [
    "trainData = OpenFile('C:\\\\Users\\\\farha\\\\Documents\\\\Python\\\\Kaggle\\\\Bosch\\\\Train_ValCnts.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bosch_PL.ipynb\n",
      "main.py\n",
      "Bosch_PL-checkpoint.ipynb\n",
      "Date_Data.pk\n",
      "Full_byDate_trainData.pk\n",
      "Numeric_Data.pk\n",
      "Response_1s.pk\n",
      "testData_datesOnly.pk\n",
      "Train_ValCnts.pk\n",
      "sample_submission.csv\n",
      "xgbsubmission.csv\n",
      "test_categorical.csv\n",
      "test_date.csv\n",
      "test_numeric.csv\n",
      "train_categorical.csv\n",
      "train_categorical_less.csv\n",
      "train_date.csv\n",
      "train_date_less.csv\n",
      "train_numeric.csv\n",
      "train_numeric_less.csv\n"
     ]
    }
   ],
   "source": [
    "for subdir, dirs, files in os.walk('./'):\n",
    "    for file in files:\n",
    "      print (file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Train_ValCnts.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-63fba5d4a4aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train_ValCnts.pk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-2696808d87d7>\u001b[0m in \u001b[0;36mOpenFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mOpenFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train_ValCnts.pk'"
     ]
    }
   ],
   "source": [
    "trainData = OpenFile(\"Train_ValCnts.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
